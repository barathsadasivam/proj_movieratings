{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlFzF7Aoqukx/NkM59Pa01"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MGr1JDC6jUQY",
        "outputId": "a5ea25cb-b679-43cf-eef9-531fd9cd5057"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/spark-3.3.4-bin-hadoop3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.3.4/spark-3.3.4-bin-hadoop3.tgz\n",
        "# Unzip the file\n",
        "!tar xf spark-3.3.4-bin-hadoop3.tgz\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = '/content/spark-3.3.4-bin-hadoop3'\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "# Check the location for Spark\n",
        "findspark.find()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import col, row_number, coalesce, lit\n",
        "from pyspark.sql import SparkSession, Window\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "import json\n",
        "\n",
        "config_file_path = \"/content/config.json\"\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "# Function to read CSV file with error handling\n",
        "def read_csv_with_error_handling(file_path, schema):\n",
        "    try:\n",
        "        return spark.read.csv(file_path, sep='::', schema=schema)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file at '{file_path}': {e}\")\n",
        "        raise e  # Raise exception in case of an error\n",
        "\n",
        "# Read configuration from JSON file\n",
        "with open(config_file_path, 'r') as config_file:\n",
        "    config = json.load(config_file)\n",
        "\n",
        "# Extract parameters from the config\n",
        "ratings_path = config['file_paths']['ratings_data']\n",
        "movies_path = config['file_paths']['movies_data']\n",
        "original_ratings_path = config['file_paths']['original_ratings_targetformat']\n",
        "original_movies_path = config['file_paths']['original_movies_targetformat']\n",
        "movie_stats_data_path = config['file_paths']['movie_stats_data']\n",
        "top_3_per_user_data_path = config['file_paths']['top_3_per_user_data']\n",
        "ratings_schemafile_path = config['file_paths']['ratings_schemafile']\n",
        "movies_schemafile_path = config['file_paths']['movies_schemafile']\n",
        "\n",
        "# Load the schema file from the local path\n",
        "with open(ratings_schemafile_path, 'r') as ratings_file:\n",
        "    ratings_schema_data = json.load(ratings_file)\n",
        "\n",
        "# Convert the schema data to StructType\n",
        "ratings_schema = StructType.fromJson(ratings_schema_data)\n",
        "\n",
        "with open(movies_schemafile_path, 'r') as movies_file:\n",
        "    movies_schema_data = json.load(movies_file)\n",
        "\n",
        "movies_schema = StructType.fromJson(movies_schema_data)\n",
        "\n",
        "# Read CSV files with error handling\n",
        "df_ratings = read_csv_with_error_handling(ratings_path, ratings_schema)\n",
        "df_movies = read_csv_with_error_handling(movies_path, movies_schema)\n",
        "\n",
        "# Replace any NULL in \"Rating\" column to 0\n",
        "df_ratings = df_ratings.na.fill(0, subset=[\"Rating\"])\n",
        "\n",
        "# Check if files were successfully read before proceeding\n",
        "if df_ratings and df_movies:\n",
        "\n",
        "  # Schema validation checks\n",
        "  if df_ratings.schema == ratings_schema:\n",
        "      print(\"Ratings data schema matches the expected schema.\")\n",
        "  else:\n",
        "      print(\"Ratings data schema does not match the expected schema.\")\n",
        "      raise Exception(\"Ratings data schema does not match the expected schema\")\n",
        "\n",
        "  if df_movies.schema == movies_schema:\n",
        "      print(\"Movies data schema matches the expected schema.\")\n",
        "  else:\n",
        "      print(\"Movies data schema does not match the expected schema.\")\n",
        "      raise Exception(\"Movies data schema does not match the expected schema\")\n",
        "\n",
        "try:\n",
        "  # statistics per MovieID\n",
        "    df_movie_stats = df_ratings.groupBy(df_ratings.MovieID) \\\n",
        "        .agg(\n",
        "            F.max(df_ratings.Rating).alias(\"MaxRating\"),\n",
        "            F.min(df_ratings.Rating).alias(\"MinRating\"),\n",
        "            F.avg(df_ratings.Rating).alias(\"AvgRating\")\n",
        "        )\n",
        "\n",
        "    # Left join movie data with aggregated statistics and handle missing values\n",
        "    df_with_stats = df_movies.alias(\"movies\").join(\n",
        "      df_movie_stats.alias(\"stats\"),\n",
        "      col(\"movies.MovieID\") == col(\"stats.MovieID\"),\n",
        "      \"left_outer\"\n",
        "      ).select(\n",
        "        col(\"movies.MovieID\"),\n",
        "        col(\"movies.Title\"),\n",
        "        col(\"movies.Genres\"),\n",
        "        coalesce(col(\"stats.MinRating\"), lit(0)).alias(\"MinRating\"),\n",
        "        coalesce(col(\"stats.MaxRating\"), lit(0)).alias(\"MaxRating\"),\n",
        "        coalesce(col(\"stats.AvgRating\"), lit(0)).alias(\"AvgRating\")\n",
        "      )\n",
        "\n",
        "    # window function and assign row number to each row within each partition\n",
        "    window_spec = Window.partitionBy(\"UserID\").orderBy(col(\"Rating\").desc())\n",
        "    df_with_row_numbers = df_ratings.withColumn(\"row_number\", row_number().over(window_spec))\n",
        "\n",
        "    # Select top 3 rows for each UserID ordered by 'row_number'\n",
        "    df_top_3_per_user = df_with_row_numbers.where(col(\"row_number\") <= 3).orderBy(\"UserID\", \"row_number\")\n",
        "\n",
        "    # Join dataframes to get the final desired output\n",
        "    final_output = df_movies.join(df_top_3_per_user, df_movies.MovieID == df_top_3_per_user.MovieID) \\\n",
        "        .select(df_top_3_per_user.UserID, df_top_3_per_user.MovieID, df_movies.Title, df_top_3_per_user.Rating, df_top_3_per_user.row_number)\n",
        "\n",
        "    # Write original and final DataFrames to Parquet format\n",
        "    df_ratings.write.mode('overwrite').parquet(original_ratings_path)\n",
        "    df_movies.write.mode('overwrite').parquet(original_movies_path)\n",
        "    df_with_stats.write.mode('overwrite').parquet(movie_stats_data_path)\n",
        "    final_output.write.mode('overwrite').parquet(top_3_per_user_data_path)\n",
        "except Exception as e:\n",
        "    print(\"Unknown exception occured - {}\".format(str(e)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNUKYvQRjdFI",
        "outputId": "a26042e3-43cd-43f0-f394-c8e3b815e795"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ratings data schema matches the expected schema.\n",
            "Movies data schema matches the expected schema.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test Cases:**"
      ],
      "metadata": {
        "id": "HR3iTz_hrj6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the number of MovieID's in the original movies.dat file matches with the number of MovieID's in the derived movie_stats parquet\n",
        "MovieID_count = df_movies.select(\"MovieID\").count()\n",
        "parquet_file_path = \"/content/output_files/movie_stats/\"\n",
        "df_movie_stats_parquet = spark.read.parquet(parquet_file_path)\n",
        "derived_movie_stats_MovieID_count = df_movie_stats_parquet.select(\"MovieID\").count()\n",
        "print(\"Count of MovieIDs in original movies.dat file:\", MovieID_count)\n",
        "print(\"Count of MovieIDs in derived movie_stats parquet:\", derived_movie_stats_MovieID_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLDJxIN8jWBs",
        "outputId": "c8b57a4c-21f6-4a76-aab4-c8a54dbcd730"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count of MovieIDs in original movies.dat file: 3883\n",
            "Count of MovieIDs in derived movie_stats parquet: 3883\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the MinRating,MaxRating and AvgRating are populated as 0's for movies not available in ratings data\n",
        "\n",
        "# Number of distinct MovieID's in the original movies data file\n",
        "distinct_MovieID_count = df_movies.select(\"MovieID\").distinct().count()\n",
        "print(\"Number of distinct MovieID's in the original movies data file:\", distinct_MovieID_count)\n",
        "\n",
        "# Number of distinct MovieID's in the original ratings data file\n",
        "ratings_distinct_MovieID_count = df_ratings.select(\"MovieID\").distinct().count()\n",
        "print(\"Number of distinct MovieID's in the original ratings data file:\", ratings_distinct_MovieID_count)\n",
        "\n",
        "# Number of movies not available in the ratings file\n",
        "diff_count = distinct_MovieID_count - ratings_distinct_MovieID_count\n",
        "print(\"Number of movies not available in the ratings data file:\", diff_count)\n",
        "\n",
        "# In the derived movie_stats parquet file, check if all these 177 movies are filled with \"0\" in the aggregation columns\n",
        "missingmovie_handled_rowcount = df_movie_stats_parquet.where((col(\"MinRating\")==0) & (col(\"MaxRating\")==0) & (col(\"AvgRating\")==0)).count()\n",
        "print(\"Number of rows handled for missing movies:\", missingmovie_handled_rowcount)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDjME5XNknb-",
        "outputId": "7941c717-2ffb-401c-d197-4c4f17769cb0"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of distinct MovieID's in the original movies data file: 3883\n",
            "Number of distinct MovieID's in the original ratings data file: 3706\n",
            "Number of movies not available in the ratings data file: 177\n",
            "Number of rows handled for missing movies: 177\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the distinct number of UserID's in the original Ratings.dat file matches with the distinct number of UserID's in the derived top3peruser parquet file\n",
        "distinct_UserID_count = df_ratings.select(\"UserID\").distinct().count()\n",
        "parquet_file_path = \"/content/output_files/top3peruser/\"\n",
        "df_top3peruser_parquet = spark.read.parquet(parquet_file_path)\n",
        "derived_top3peruser_distinct_UserID_count = df_top3peruser_parquet.select(\"UserID\").distinct().count()\n",
        "print(\"Count of distinct UserIDs in original Ratings.dat file:\", distinct_UserID_count)\n",
        "print(\"Count of distinct UserIDs in derived top3peruser parquet:\", derived_top3peruser_distinct_UserID_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMgmwXl9o49S",
        "outputId": "3ff0cd53-764e-44d5-deee-197b620c8bdc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count of distinct UserIDs in original Ratings.dat file: 6040\n",
            "Count of distinct UserIDs in derived top3peruser parquet: 6040\n"
          ]
        }
      ]
    }
  ]
}